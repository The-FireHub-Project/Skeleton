#
# This file is part of the FireHub Project ecosystem
#
# Robots.txt is a text file webmasters create to instruct web robots (typically search engine robots) how to crawl pages
# on their website.
# @since 1.0.0
#
# @author Danijel GaliÄ‡ <danijel.galic@outlook.com>
# @copyright 2026 The FireHub Project - All rights reserved
# @license https://opensource.org/licenses/MIT MIT License
#
# @package App\Web
#
# @version GIT: $Id$ Blob checksum.
#

User-agent: *
Disallow: